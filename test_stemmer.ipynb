{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = \"Such an analysis can reveal features that are not easily visible from the \"\\\n",
    "    + \"varitions in the individual genes and can lead to a picture of expression \"\\\n",
    "    + \"that is more biologically transparent and accessible\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Such',\n",
       " 'an',\n",
       " 'analysis',\n",
       " 'can',\n",
       " 'reveal',\n",
       " 'features',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'easily']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_num = 10\n",
    "tokens = nltk.tokenize.word_tokenize(txt)\n",
    "tokens[:print_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word      porter   landcaster regexp\n",
      "Such      Such      such      Such\n",
      "an        an        an        an\n",
      "analysis  analysi  analys  analysi\n",
      "can       can       can       can\n",
      "reveal    reveal    rev    reveal\n",
      "features  featur  feat  feature\n",
      "that      that      that      that\n",
      "are       are       ar       are\n",
      "not       not       not       not\n",
      "easily    easili    easy    easily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# porter\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "# Lancaster (Paice/Husk)\n",
    "landcaster = nltk.stem.LancasterStemmer()\n",
    "# Regular expressions uses regular expressions that should be written by user\n",
    "regexp = nltk.stem.regexp.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "print(\"word     \", \"porter  \", \"landcaster\", \"regexp\")\n",
    "[print(word, porter.stem(word), landcaster.stem(word), regexp.stem(word), sep=\" \"*(10-len(word))) \\\n",
    " for word in tokens[:print_num]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word      wn_lemmatize\n",
      "Such      Such\n",
      "an        an\n",
      "analysis  analysis\n",
      "can       can\n",
      "reveal    reveal\n",
      "features  feature\n",
      "that      that\n",
      "are       are\n",
      "not       not\n",
      "easily    easily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordnet lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "print(\"word     \", \"wn_lemmatize\")\n",
    "[print(word, wnl.lemmatize(word), sep=\" \"*(10-len(word))) \\\n",
    " for word in tokens[:print_num]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word      porter   landcaster regexp    wn_lemmatize\n",
      "Such      Such      such      Such      Such\n",
      "an        an        an        an        an\n",
      "analysis  analysi  analys  analysi  analysis\n",
      "can       can       can       can       can\n",
      "reveal    reveal    rev    reveal    reveal\n",
      "features  featur  feat  feature  feature\n",
      "that      that      that      that      that\n",
      "are       are       ar       are       are\n",
      "not       not       not       not       not\n",
      "easily    easili    easy    easily    easily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# porter\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "# Lancaster (Paice/Husk)\n",
    "landcaster = nltk.stem.LancasterStemmer()\n",
    "# Regular expressions uses regular expressions that should be written by user\n",
    "regexp = nltk.stem.regexp.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "# wordnet lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "print(\"word     \", \"porter  \", \"landcaster\", \"regexp   \", \"wn_lemmatize\")\n",
    "[print(word, \\\n",
    "       porter.stem(word), landcaster.stem(word), regexp.stem(word), wnl.lemmatize(word),\\\n",
    "       sep=\" \"*(10-len(word))) \\\n",
    " for word in tokens[:print_num]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (doc.py, line 175)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"C:\\Users\\stard\\Anaconda3\\lib\\site-packages\\stanfordnlp\\pipeline\\doc.py\"\u001b[0;36m, line \u001b[0;32m175\u001b[0m\n\u001b[0;31m    return f\"<{self.__class__.__name__} index={self.index};words={self.words}>\"\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# stanfordnlp supports python 3.6.8+ or 3.7.2\n",
    "import stanfordnlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Such',\n",
       " 'an',\n",
       " 'analysis',\n",
       " 'can',\n",
       " 'reveal',\n",
       " 'features',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'easily']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_num = 10\n",
    "tokens = nltk.tokenize.word_tokenize(txt)\n",
    "tokens[:print_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
